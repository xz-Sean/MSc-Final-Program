{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c6c6f08",
   "metadata": {},
   "source": [
    "# Copula Synthesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "fde2496f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from statsmodels.distributions.copula.api import (\n",
    "    GaussianCopula, StudentTCopula, ClaytonCopula,\n",
    "    CopulaDistribution)\n",
    "from scipy.stats import norm\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from statsmodels.stats.correlation_tools import corr_nearest\n",
    "from numpy.linalg import eigh, cholesky, LinAlgError, slogdet, solve\n",
    "import scipy.stats as st\n",
    "from scipy.special import gammaln\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import roc_auc_score, mean_squared_error\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b149ce21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hold-out test set saved to: data/merged_data/test_set.csv\n",
      "Hold-out test set and train set are saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xz2821/anaconda3/envs/IRP/lib/python3.8/site-packages/sklearn/preprocessing/_data.py:2590: UserWarning: n_quantiles (1000) is greater than the total number of samples (209). n_quantiles is set to n_samples.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Data ingestion and marginal uniformisation\n",
    "\n",
    "# 1. Columns containing string identifiers or metadata that are NOT\n",
    "#    relevant for dependence modelling. We drop them before fitting the copula.\n",
    "drop_cols = ['uid',          # unique identifier (string)\n",
    "             'dataset',      # data-set source label\n",
    "             'group']        # FI-based categorical group label\n",
    "\n",
    "# 2. Read the numeric source table and remove the non-numeric columns in a single chained operation.\n",
    "# Save test set data individually\n",
    "df_full = (pd.read_excel(\"data/merged_data/fi_pool.xlsx\")\n",
    "      .drop(columns=drop_cols)                       # keep only numeric features\n",
    ")\n",
    "df, df_test = train_test_split(    \n",
    "    df_full,\n",
    "    test_size=0.20,\n",
    "    random_state=42,\n",
    "    shuffle=True\n",
    ")\n",
    "test_out = Path(\"data/merged_data/test_set.csv\")\n",
    "test_out.parent.mkdir(parents=True, exist_ok=True)\n",
    "df_test.to_csv(test_out, index=False)\n",
    "print(f\"Hold-out test set saved to: {test_out}\")\n",
    "\n",
    "train_out = Path(\"data/merged_data/train_set.csv\")\n",
    "train_out.parent.mkdir(parents=True, exist_ok=True)\n",
    "df.to_csv(train_out, index=False)\n",
    "print(f\"Hold-out test set and train set are saved.\")\n",
    "\n",
    "# 3. Convert each marginal distribution to U(0,1) via the\n",
    "#    probability-integral transform (a.k.a. \"pseudo-observations\").\n",
    "qt = QuantileTransformer(\n",
    "        output_distribution='uniform',\n",
    "     )\n",
    "\n",
    "# fit_transform():\n",
    "# fit: estimate the empirical CDF of each column.\n",
    "# transform: map every x_ij to u_ij = F_j(x_ij) to (0,1).\n",
    "u = pd.DataFrame(\n",
    "        qt.fit_transform(df),\n",
    "        columns=df.columns        # preserve original feature names\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "c66c1652",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a numerically-stable Gaussian-copula correlation matrix rho (with shrinkage + PD repair)\n",
    "\n",
    "# 1. Guard-band the pseudo-observations \n",
    "eps = 1e-3                                  # 0.1% safety margin\n",
    "u_clip = u.clip(lower=eps, upper=1 - eps)   # keep inside (eps, 1-eps)\n",
    "\n",
    "# 2. Rank-correlation \n",
    "tau = u_clip.corr(method='kendall').to_numpy()         # d × d matrix\n",
    "rho = np.sin(0.5 * np.pi * tau)                        # same shape\n",
    "\n",
    "# 3. shrinkage toward the identity \n",
    "# Small samples or highly-collinear features can make rho nearly singular. \n",
    "# Shrinkage lifts all eigen-values and stabilises log(rho) in the log-likelihood.\n",
    "lam  = 0.05                                            # 5% shrinkage\n",
    "rho  = (1 - lam) * rho + lam * np.eye(rho.shape[0])    # convex blend\n",
    "\n",
    "# 4. Ensure ρ is strictly positive-definite \n",
    "try:\n",
    "    _ = cholesky(rho)                 # PD quick-check\n",
    "except np.linalg.LinAlgError:\n",
    "    # Spectral repair: floor eigen-values to a small positive constant.\n",
    "    w, v = eigh(rho)                  \n",
    "    w[w < 1e-6] = 1e-6                # eigen-value floor\n",
    "    rho = v @ np.diag(w) @ v.T        # nearest PD matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "b79e93ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gaussian Copula  AIC = 1,712.19\n"
     ]
    }
   ],
   "source": [
    "# Log-likelihood of the Gaussian copula  and the AIC score\n",
    "\n",
    "# 1. Preparations \n",
    "d = u.shape[1]  # dimensionality of the feature space\n",
    "\n",
    "# Transform the clipped uniform pseudo-observations back to the standard-normal space\n",
    "z = norm.ppf(u_clip.to_numpy())             # shape: n × d (n = row-count)\n",
    "\n",
    "# Pre-compute the inverse and the log-determinant of the correlation\n",
    "# matrix  Sigma  (here, Sigma ≡ rho).\n",
    "Sigma_inv = solve(rho, np.eye(d))          # Sigma^{-1}\n",
    "sign, logdet_Sigma = slogdet(rho)          # log |Sigma|\n",
    "quad = (z @ Sigma_inv * z).sum(axis=1)     # shape: (n,)\n",
    "\n",
    "# 2. Row-wise log-density of the Gaussian copula \n",
    "log_joint = -0.5 * quad - 0.5 * logdet_Sigma          # log phi_{Sigma}(z_i)\n",
    "log_marg  = norm.logpdf(z).sum(axis=1)                # Sigma_j log phi(z_{ij})\n",
    "logpdf    = log_joint - log_marg                      # log c(u_i) for each row\n",
    "\n",
    "# 3. Total maximised log-likelihood\n",
    "log_like = logpdf.sum()                               #  l_max  = Sigma_i log c(u_i)\n",
    "\n",
    "# 4. Akaike Information Criterion\n",
    "# AIC = 2*k  −  2*l_max,\n",
    "k_params = d * (d - 1) / 2\n",
    "AIC = 2 * k_params - 2 * log_like\n",
    "\n",
    "print(f\"Gaussian Copula  AIC = {AIC:,.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "9dd255d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synthetic data written to:  synthetic_gauss.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xz2821/anaconda3/envs/IRP/lib/python3.8/site-packages/sklearn/base.py:450: UserWarning: X does not have valid feature names, but QuantileTransformer was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Draw synthetic samples from the fitted Gaussian copula and map them back to the original measurement scale\n",
    "\n",
    "# 1. Instantiate the copula for sampling.\n",
    "gc = GaussianCopula(corr=rho, k_dim=d)\n",
    "\n",
    "# 2. Random-variates generation (n = 5 000 rows).\n",
    "u_syn = gc.rvs(5_000, random_state=42)                 # reproducible RNG\n",
    "u_syn = u_syn.clip(eps, 1 - eps)                       # keep inside (sigma, 1-sigma)\n",
    "\n",
    "# 3. Reverse the probability-integral transform to recover the original physical units for every feature.  \n",
    "x_syn = pd.DataFrame(\n",
    "            qt.inverse_transform(u_syn),\n",
    "            columns=df.columns                         # restore feature names\n",
    "        )\n",
    "\n",
    "# 4. Persist the “coarse” synthetic data set to disk.\n",
    "out_path = \"synthetic_gauss.csv\"\n",
    "x_syn.to_csv(out_path, index=False)\n",
    "print(f\"Synthetic data written to:  {out_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d9f5c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Student-t Copula  (v = 8)   AIC = 1,345.29\n",
      "Synthetic data written to:  synthetic_t8.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xz2821/anaconda3/envs/IRP/lib/python3.8/site-packages/sklearn/base.py:450: UserWarning: X does not have valid feature names, but QuantileTransformer was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Student-t Copula (with correlation-matrix shrinkage already applied)\n",
    "\n",
    "# 1. Maximum-likelihood search over degrees-of-freedom v\n",
    "best_ll  = -np.inf          # best log-likelihood so far\n",
    "best_df  = None             # v that achieves it\n",
    "z_cache  = {}               # reuse t.ppf(U) per v (n × d arrays)\n",
    "\n",
    "for nu in [3, 4, 6, 8, 12, 20]:\n",
    "    # 1a. Map U(0,1) pseudo-obs back to t_v quantiles\n",
    "    if nu not in z_cache:\n",
    "        z_cache[nu] = st.t.ppf(u_clip, df=nu)      # ndarray (n, d)\n",
    "    z = z_cache[nu]\n",
    "\n",
    "    # 1b. Pre-compute pieces used in every row\n",
    "    Sigma_inv = solve(rho, np.eye(d))      \n",
    "    _, logdet_Sigma = slogdet(rho)               \n",
    "    quad = (z @ Sigma_inv * z).sum(axis=1) \n",
    "\n",
    "    # 1c. Multivariate-t normalising constant (log-space)\n",
    "    log_const = (\n",
    "        gammaln((nu + d) / 2)           \n",
    "        - gammaln(nu / 2)               \n",
    "        - 0.5 * logdet_Sigma\n",
    "        - (d / 2) * np.log(nu * np.pi)\n",
    "    )\n",
    "\n",
    "    # 1d. Row-wise joint log-density of the copula\n",
    "    log_joint = log_const - (nu + d) / 2 * np.log1p(quad / nu)\n",
    "\n",
    "    # 1e. Sum of 1-D marginal t log-densities\n",
    "    log_marg  = st.t.logpdf(z, df=nu).sum(axis=1)        # (n,)\n",
    "\n",
    "    # 1f. Copula log-pdf = joint − marginal -> total log-likelihood\n",
    "    ll = (log_joint - log_marg).sum()\n",
    "\n",
    "    if ll > best_ll:          \n",
    "        best_ll, best_df = ll, nu\n",
    "\n",
    "\n",
    "# 2. Information criterion\n",
    "# k = d(d−1)/2  (ρ parameters)  + 1  (v)\n",
    "k_t   = d * (d - 1) // 2 + 1\n",
    "AIC_t = 2 * k_t - 2 * best_ll\n",
    "print(f\"Student-t Copula  (v = {best_df})   AIC = {AIC_t:,.2f}\")\n",
    "\n",
    "\n",
    "# 3. Generate synthetic data and map back to real units\n",
    "tc      = StudentTCopula(rho, df=best_df, k_dim=d)\n",
    "u_syn   = tc.rvs(5_000, random_state=42).clip(eps, 1 - eps)\n",
    "x_syn_t = pd.DataFrame(qt.inverse_transform(u_syn), columns=df.columns)\n",
    "\n",
    "out_path = f\"synthetic_t{best_df}.csv\"\n",
    "x_syn_t.to_csv(out_path, index=False)\n",
    "print(f\"Synthetic data written to:  {out_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "dc3de11a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clayton theta (from Kendall tau) = 0.274\n",
      "Clayton Copula  AIC = 535.20\n",
      "Synthetic data written to:  synthetic_clayton.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xz2821/anaconda3/envs/IRP/lib/python3.8/site-packages/sklearn/base.py:450: UserWarning: X does not have valid feature names, but QuantileTransformer was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Clayton Copula \n",
    "\n",
    "# 1. One–parameter estimation\n",
    "theta_hat = ClaytonCopula().fit_corr_param(u_clip) \n",
    "print(f\"Clayton theta (from Kendall tau) = {theta_hat:.3f}\")\n",
    "\n",
    "# 2. Build the copula object\n",
    "# k_dim = d  (number of marginals); \n",
    "clay = ClaytonCopula(theta=theta_hat, k_dim=d)\n",
    "\n",
    "# 3. Log-likelihood and AIC\n",
    "log_like_clay = clay.logpdf(u_clip).sum()      # l_max (scalar)\n",
    "k_clay   = 1                               \n",
    "AIC_clay = 2 * k_clay - 2 * log_like_clay\n",
    "print(f\"Clayton Copula  AIC = {AIC_clay:,.2f}\")\n",
    "\n",
    "# 4. Sampling + inverse marginal transform\n",
    "u_syn = clay.rvs(5_000, random_state=42).clip(eps, 1 - eps)\n",
    "x_syn_clay = pd.DataFrame(\n",
    "                 qt.inverse_transform(u_syn),\n",
    "                 columns=df.columns\n",
    "              )\n",
    "\n",
    "out_path = \"synthetic_clayton.csv\"\n",
    "x_syn_clay.to_csv(out_path, index=False)\n",
    "print(f\"Synthetic data written to:  {out_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "bc0eb60c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Copula model comparison (sorted by per-row AIC) ---\n",
      "             model    AIC_total  AIC_per_row  LL_per_row  k_params\n",
      "0          Clayton   535.204910     2.560789   -1.275610       1.0\n",
      "1  Student-t(nu=8)  1345.290369     6.436796   -3.079642      29.0\n",
      "2         Gaussian  1712.185518     8.192275   -3.962166      28.0\n",
      "\n",
      "Comparison table saved to: data/Copula_data/copula_comparison.csv\n"
     ]
    }
   ],
   "source": [
    "# Comparison :  Gaussian  vs.  Student-t  vs.  Clayton\n",
    "\n",
    "# 1. Gather raw numbers\n",
    "metrics = {\n",
    "    \"model\"        : [\"Gaussian\",      f\"Student-t(nu={best_df})\", \"Clayton\"],\n",
    "    \"AIC_total\"    : [AIC,             AIC_t,                     AIC_clay],\n",
    "    \"loglike_total\": [log_like,        best_ll,                   log_like_clay],\n",
    "    \"k_params\"     : [k_params,        k_t,                       k_clay]\n",
    "}\n",
    "\n",
    "# 2. Scale-free normalisation (per training row)\n",
    "n_train = len(u_clip)                 # number of rows used for fitting\n",
    "metrics[\"AIC_per_row\"]   = [v / n_train for v in metrics[\"AIC_total\"]]\n",
    "metrics[\"LL_per_row\"]    = [v / n_train for v in metrics[\"loglike_total\"]]\n",
    "\n",
    "tbl = pd.DataFrame(metrics)\\\n",
    "         .sort_values(\"AIC_per_row\")\\\n",
    "         .reset_index(drop=True)\n",
    "\n",
    "print(\"\\n--- Copula model comparison (sorted by per-row AIC) ---\")\n",
    "print(tbl[[\"model\", \"AIC_total\", \"AIC_per_row\", \"LL_per_row\", \"k_params\"]])\n",
    "\n",
    "# 3. Save comparison table to disk \n",
    "out_cmp = Path(\"data/Copula_data/copula_comparison.csv\")\n",
    "tbl.to_csv(out_cmp, index=False)\n",
    "print(f\"\\nComparison table saved to: {out_cmp}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aac0311",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "Basing on the result, **Clayton Copula** perform much better in our circumstance. We can do a data quality check on the generated data of Clayton Copula.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e098d5b",
   "metadata": {},
   "source": [
    "# Data Quality Checking of Copula Synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "3bbd8d7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RULE] spacing_over_burden ≥ 1  violated in 0 / 5000 rows\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Full QC pipeline for Gaussian-copula synthetic data.\n",
    "Assumes you already have:\n",
    "    df_real   : pandas DataFrame of cleaned real data (numeric only)\n",
    "    x_syn     : pandas DataFrame of synthetic data from Gaussian copula\n",
    "Both must have identical columns in the same order.\n",
    "\"\"\"\n",
    "# Basic settings\n",
    "plt.rcParams[\"figure.dpi\"] = 120\n",
    "out_dir = \"qc_reports/\"\n",
    "Path(out_dir).mkdir(exist_ok=True)\n",
    "\n",
    "# Real data  \n",
    "raw_path  = \"data/merged_data/fi_pool.xlsx\"          \n",
    "drop_cols = ['uid', 'dataset', 'group']     \n",
    "\n",
    "df_real = (\n",
    "    pd.read_excel(raw_path)\n",
    "      .drop(columns=drop_cols)                      \n",
    ")\n",
    "\n",
    "# Synthetic data  (already generated by Gaussian copula)\n",
    "syn_path = \"data/Copula_data/synthetic_clayton.csv\"   \n",
    "x_syn    = pd.read_csv(syn_path)\n",
    "\n",
    "# Helper: save a figure then close\n",
    "def save_fig(name): plt.tight_layout(); plt.savefig(f\"{out_dir}/{name}\", bbox_inches=\"tight\"); plt.close()\n",
    "\n",
    "\n",
    "# 1. Univariate KS / AD + overlay histogram\n",
    "ks_results = {}\n",
    "for col in df_real.columns:\n",
    "    # KS statistic\n",
    "    D, p = st.ks_2samp(df_real[col], x_syn[col])\n",
    "    ks_results[col] = D\n",
    "    \n",
    "    # Overlay histogram\n",
    "    plt.hist(df_real[col], bins=40, density=True, alpha=.4, label=\"real\")\n",
    "    plt.hist(x_syn[col] , bins=40, density=True, alpha=.4, label=\"synthetic\")\n",
    "    plt.title(f\"Histogram - {col}  (KS={D:.3f})\")\n",
    "    plt.legend()\n",
    "    save_fig(f\"hist_{col}.png\")\n",
    "\n",
    "# Save KS table\n",
    "pd.Series(ks_results, name=\"KS\").to_csv(f\"{out_dir}/ks_univariate.csv\")\n",
    "\n",
    "# 2. Correlation heat-map\n",
    "tau_real = df_real.corr(\"kendall\")\n",
    "tau_syn  = x_syn.corr(\"kendall\")\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 4))\n",
    "sns.heatmap(tau_real, vmin=-1, vmax=1, cmap=\"coolwarm\", ax=ax[0])\n",
    "ax[0].set_title(\"Real Kendall τ\")\n",
    "sns.heatmap(tau_syn , vmin=-1, vmax=1, cmap=\"coolwarm\", ax=ax[1])\n",
    "ax[1].set_title(\"Synthetic Kendall τ\")\n",
    "save_fig(\"tau_heatmap.png\")\n",
    "\n",
    "# 3. Tail-dependence (upper & lower 1 %) for each variable pair\n",
    "def empirical_tail_coef(a, b, tail=0.01, upper=True):\n",
    "    if upper:\n",
    "        q_a, q_b = np.quantile(a, 1-tail), np.quantile(b, 1-tail)\n",
    "        return ((a > q_a) & (b > q_b)).mean() / tail\n",
    "    else:\n",
    "        q_a, q_b = np.quantile(a, tail), np.quantile(b, tail)\n",
    "        return ((a < q_a) & (b < q_b)).mean() / tail\n",
    "\n",
    "pairs = [(df_real.columns[i], df_real.columns[j]) \n",
    "         for i in range(len(df_real.columns)) for j in range(i+1, len(df_real.columns))]\n",
    "records = []\n",
    "for c1, c2 in pairs:\n",
    "    lam_L_real = empirical_tail_coef(df_real[c1], df_real[c2], upper=False)\n",
    "    lam_L_syn  = empirical_tail_coef(x_syn[c1] , x_syn[c2] , upper=False)\n",
    "    records.append([c1, c2, lam_L_real, lam_L_syn])\n",
    "\n",
    "pd.DataFrame(records, columns=[\"var1\",\"var2\",\"lambda_L_real\",\"lambda_L_syn\"])\\\n",
    "  .to_csv(f\"{out_dir}/tail_coef_lower.csv\", index=False)\n",
    "\n",
    "# 4. PCA 2-D projection\n",
    "pca = PCA(n_components=2, random_state=42)\n",
    "pca_real = pca.fit_transform(df_real)\n",
    "pca_syn  = pca.transform(x_syn)\n",
    "\n",
    "plt.scatter(*pca_real.T, s=6, alpha=.4, label=\"real\")\n",
    "plt.scatter(*pca_syn.T , s=6, alpha=.4, label=\"synthetic\")\n",
    "plt.title(\"PCA - first 2 components\")\n",
    "plt.legend(); save_fig(\"pca_2d.png\")\n",
    "\n",
    "#  5.  Physcial rule check  (example: burden ≤ spacing)\n",
    "ratio_col = \"spacing_over_burden\"        # name of the ratio column\n",
    "\n",
    "# Count rows that violate  spacing_over_burden ≥ 1\n",
    "violations = (x_syn[ratio_col] < 1).sum()\n",
    "print(f\"[RULE] {ratio_col} ≥ 1  violated in {violations} / {len(x_syn)} rows\")\n",
    "\n",
    "# Optional hard-filter: keep only rows that satisfy the rule\n",
    "x_syn = x_syn.query(f\"{ratio_col} >= 1\").reset_index(drop=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IRP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
